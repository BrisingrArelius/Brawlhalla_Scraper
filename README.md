# Brawlhallaâ€¯Leaderboardâ€¯Scraper

Scrapes Brawlhalla 1â€‘vâ€‘1 leaderboards and writes ratings to **CSV** files that are easy to analyse or plot later.

* Regional scraper â‡’ `src/Region/scrape_brawlhalla.py`
* Global (multiâ€‘region) scraper â‡’ `src/Global/scrape_global_regions.py`
* Retry helpers â‡’ automatically reâ€‘query pages listed in `failed_pages*.txt`
* `plot_csv_hist.py` â‡’ example Matplotlib histogram (optional)

Each run stores its output in a dateâ€‘stamped folder (e.g. `Data-22Jun/`) so nothing is overwritten.

---

## 1ï¸âƒ£  Quickâ€‘start

```bash
# clone & enter the repo
git clone https://github.com/BrisingrArelius/Brawlhalla_Scraper.git
cd Brawlhalla_Scraper

# create + activate a virtual environment
python3 -m venv .venv
source .venv/bin/activate          # Windows: .venv\Scripts\Activate.ps1

# install dependencies (just `requests` for now)
pip install -r requirements.txt
```

Thatâ€™s all the setup you need.

---

## 2ï¸âƒ£  Scraping just one region

The **regional** scraper defaults to `SEA`, but you can pick any region listed on the official website.

```bash
# Default (SEA)
python src/Region/scrape_brawlhalla.py

# EU example
python src/Region/scrape_brawlhalla.py --region eu
```

### What you get

```
Data-22Jun/
â”œâ”€â”€ peak_ratings_sea.csv
â”œâ”€â”€ season_ratings_sea.csv
â”œâ”€â”€ failed_pages.txt       â† pages that 404 or timeâ€‘out
â””â”€â”€ last_page.txt          â† resume checkpoint
```

Running the scraper again resumes automatically and never duplicates rows.

---

## 3ï¸âƒ£  Scraping the global leaderboard

```bash
python src/Global/scrape_global_regions.py
```

This pulls *all* regions in one go and writes one pair of CSVs **per region** plus a combined "global" pair.

```
Data-22Jun/
â”œâ”€â”€ peak_ratings_global.csv
â”œâ”€â”€ season_ratings_global.csv
â”œâ”€â”€ peak_ratings_EU.csv
â”œâ”€â”€ season_ratings_EU.csv
â”œâ”€â”€ â€¦
â”œâ”€â”€ failed_pages_global.txt
â””â”€â”€ last_page_global.txt
```

---

## 4ï¸âƒ£  Retrying failed pages

If the connection hiccups you can reâ€‘query just the failed pages instead of rerunning the whole scraping job:

```bash
# for a singleâ€‘region run
python src/Region/retry_failed_pages.py

# for the global run
python src/Global/retry_failed_pages_global.py
```

Each script tries up to **3** times per page and updates the CSVs inâ€‘place.

---

## 5ï¸âƒ£  Plotting example

After scraping you can visualise the distribution, e.g.:

```bash
python plot_csv_hist.py Data-22Jun/peak_ratings_sea.csv
```

This is purely optionalâ€”feel free to plug the CSVs into Excel, Pandas, or anything else.

---

## 6ï¸âƒ£  Customisation

| Setting                            | How to change                                      | Default                      |
| ---------------------------------- | -------------------------------------------------- | ---------------------------- |
| **Region** (singleâ€‘region scraper) | `--region eu` or edit `REGION` in the config block | `sea`                        |
| Output folder name                 | edit the `DATA_DIR` definition                     | `Data-<today>`               |
| Request timeout                    | `TIMEOUT_S` constant                               | 12Â s                         |
| Delay between requests             | `PAUSE_S` constant                                 | 0Â s (global) / 0.5Â s (retry) |
| Max empty pages before stop        | `STOP_AFTER` constant                              | 10                           |

---

## 7ï¸âƒ£  Troubleshooting

* \`\` â€“ install Python â‰¥3.8 from your package manager.
* **Authentication errors when pushing** â€“ use an SSH remote or PAT; see the Git commands in the conversation above.
* \`\` â€“ youâ€™re running an old script version; update and make sure both `region` *and* `page` placeholders are filled.

---

## 8ï¸âƒ£  Licence

MITÂ License â€“ see `LICENSE` for details.

Happy scraping! ğŸ®
